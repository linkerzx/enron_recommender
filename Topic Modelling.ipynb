{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "validPreprocessed = pickle.load(open( \"validPreprocessed.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Initialize the dataset for LDA training as a list of strings\n",
    "\"\"\"\n",
    "init_data = [x['body'] for x in validPreprocessed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Get the set of the 5% Most commonly occuring words in the dataset\n",
    "\"\"\"\n",
    "init_FreqDist = pickle.load(  open( \"init_FreqDist.p\", \"rb\" ) )\n",
    "num_words = len(init_FreqDist.most_common())\n",
    "end_wcnt = num_words / 20 #5% top windsorization\n",
    "common_words = init_FreqDist.most_common()[0:end_wcnt]\n",
    "common_words_set = set(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st = LancasterStemmer()\n",
    "\n",
    "#samples = random.sample(xrange(len(init_data)), 2000)\n",
    "samples = xrange(0, len(init_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n",
      "151000\n",
      "152000\n",
      "153000\n",
      "154000\n",
      "155000\n",
      "156000\n",
      "157000\n",
      "158000\n",
      "159000\n",
      "160000\n",
      "161000\n",
      "162000\n",
      "163000\n",
      "164000\n",
      "165000\n",
      "166000\n",
      "167000\n",
      "168000\n",
      "169000\n",
      "170000\n",
      "171000\n",
      "172000\n",
      "173000\n",
      "174000\n",
      "175000\n",
      "176000\n",
      "177000\n",
      "178000\n",
      "179000\n",
      "180000\n",
      "181000\n",
      "182000\n",
      "183000\n",
      "184000\n",
      "185000\n",
      "186000\n",
      "187000\n",
      "188000\n",
      "189000\n",
      "190000\n",
      "191000\n",
      "192000\n",
      "193000\n",
      "194000\n",
      "195000\n",
      "196000\n",
      "197000\n",
      "198000\n",
      "199000\n",
      "200000\n",
      "201000\n",
      "202000\n",
      "203000\n",
      "204000\n",
      "205000\n",
      "206000\n",
      "207000\n",
      "208000\n",
      "209000\n",
      "210000\n",
      "211000\n",
      "212000\n",
      "213000\n",
      "214000\n",
      "215000\n",
      "216000\n",
      "217000\n",
      "218000\n",
      "219000\n",
      "220000\n",
      "221000\n",
      "222000\n",
      "223000\n",
      "224000\n",
      "225000\n",
      "226000\n",
      "227000\n",
      "228000\n",
      "229000\n",
      "230000\n",
      "231000\n",
      "232000\n",
      "233000\n",
      "234000\n",
      "235000\n",
      "236000\n",
      "237000\n",
      "238000\n",
      "239000\n",
      "240000\n",
      "241000\n",
      "242000\n",
      "243000\n",
      "244000\n",
      "245000\n",
      "246000\n",
      "247000\n",
      "248000\n",
      "249000\n",
      "250000\n",
      "251000\n",
      "252000\n",
      "253000\n",
      "254000\n",
      "255000\n",
      "256000\n",
      "257000\n",
      "258000\n",
      "259000\n",
      "260000\n",
      "261000\n",
      "262000\n",
      "263000\n",
      "264000\n",
      "265000\n",
      "266000\n",
      "267000\n",
      "268000\n",
      "269000\n",
      "270000\n",
      "271000\n",
      "272000\n",
      "273000\n",
      "274000\n",
      "275000\n",
      "276000\n",
      "277000\n",
      "278000\n",
      "279000\n",
      "280000\n",
      "281000\n",
      "282000\n",
      "283000\n",
      "284000\n",
      "285000\n",
      "286000\n",
      "287000\n",
      "288000\n",
      "289000\n",
      "290000\n",
      "291000\n",
      "292000\n",
      "293000\n",
      "294000\n",
      "295000\n",
      "296000\n",
      "297000\n",
      "298000\n",
      "299000\n",
      "300000\n",
      "301000\n",
      "302000\n",
      "303000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Preprocess the training sample removing common words, \n",
    "    Stemming words and Removing Special Characters\n",
    "\"\"\"\n",
    "import re\n",
    "data = []\n",
    "for i in samples:\n",
    "    # remove common words \n",
    "    # stem words\n",
    "    # Remove Special Characters\n",
    "    if i % 1000 == 0:\n",
    "        print i\n",
    "    processed_data = [\n",
    "        st.stem(re.sub('[^A-Za-z0-9]+', ' ', x)) \n",
    "        for x in init_data[i].split(\" \")\n",
    "        if (\n",
    "            x not in common_words_set\n",
    "        )\n",
    "    ] \n",
    "    append_data = ' '.join(processed_data)\n",
    "    data.append(append_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data, open( \"data_for_processing.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df=0.05, max_df=.95, stop_words='english')\n",
    "X = vec.fit_transform(data)\n",
    "vocab = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:508: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
       "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
       "             mean_change_tol=0.001, n_jobs=1, n_topics=10, perp_tol=0.1,\n",
       "             random_state=None, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Trains the LDA Model on 10 Topics\n",
    "\"\"\"\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_topics=10)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(lda, open( \"lda.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: wil thi hav meet ar op th man cal clos discuss plan friday work thes cur continu quest week risk\n",
      "Topic #1: pm date houston time schedule hour eb contact dat fax start phone john serv th help support information nee mark\n",
      "Topic #2: trad stat fin hav ar deal contract wer oth agr thi buy point thos whil mad issu thes vary ther\n",
      "Topic #3: week right year thi ev way mor pay tim aft start al end ar dur tak day ov lik mak\n",
      "Topic #4: hav know let ar thi want just lik think nee work don going som good look ther yo talk mak\n",
      "Topic #5: yo pleas thi hav ar email mail wil attach send receiv inform mess cal contact thank help list address cop\n",
      "Topic #6: thanks group busy serv provid request inform chang employ form al includ nee dat new work process produc oth develop\n",
      "Topic #7: gas market pric rat nat said california term gen reg fil cost high bil energy commit ar custom und produc\n",
      "Topic #8: enron company energy pow corp said new report busy cent year serv plan houston man wil al group execut bil\n",
      "Topic #9: com new day credit report ll today click fre spec ad sit al check chang web mail produc gre serv\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "    Outputs the top words by topic\n",
    "\"\"\"\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print_top_words(lda, vocab, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00045847e-01,   7.71159702e+03,   1.96925598e+04, ...,\n",
       "          2.27453075e+04,   7.14050190e+01,   1.20270469e-01],\n",
       "       [  1.00037435e-01,   2.95417118e+02,   1.61458095e+00, ...,\n",
       "          1.00020680e-01,   1.00006903e-01,   1.00017796e-01],\n",
       "       [  1.44503604e+04,   1.00025484e-01,   4.27546554e+03, ...,\n",
       "          8.86100995e+02,   6.46651183e+01,   1.00019571e-01],\n",
       "       ..., \n",
       "       [  1.00026235e-01,   1.00031754e-01,   8.05553590e+03, ...,\n",
       "          1.00026804e-01,   6.65828677e+03,   1.00018334e-01],\n",
       "       [  3.26816871e+03,   9.34324042e+03,   5.63381130e+03, ...,\n",
       "          1.00032776e-01,   2.62868830e+04,   1.00022551e-01],\n",
       "       [  9.33434969e+02,   2.77966609e+03,   3.23357288e+01, ...,\n",
       "          1.00022657e-01,   1.00020204e-01,   1.00031577e-01]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ldaTopicPredictFromInputArray(lda, input_array):\n",
    "    import numpy as np\n",
    "    ldaScore = lda.score(input_array)\n",
    "    array_product = input_array.reshape(1, -1) * lda.components_\n",
    "    return np.argmax(np.sum(array_product ,axis=1))\n",
    "\n",
    "def convertTextoLDAArray(text, vocab):\n",
    "    vocabDictionary = dict([(x[1], x[0]) for x in enumerate(vocab)])\n",
    "    processed_text_tokens = [\n",
    "        st.stem(re.sub('[^A-Za-z0-9]+', ' ', x)) \n",
    "        for x in text.split(\" \")\n",
    "    ]\n",
    "    output_array = np.zeros((len(vocab)))\n",
    "    for x in processed_text_tokens:\n",
    "        idxs = vocabDictionary.get(x)\n",
    "        if idxs:\n",
    "            output_array[idxs] = 1\n",
    "    return output_array.reshape(1, -1)\n",
    "\n",
    "def ldaTopicPredict(lda, text, vocab):\n",
    "    return ldaTopicPredictFromInputArray(lda,convertTextoLDAArray(text, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaTopicPredict(lda, \"Market Wolf contact email please\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    infer topic for every email in the dataset\n",
    "\"\"\"\n",
    "for x in xrange(0, len(validPreprocessed)):\n",
    "    validPreprocessed[x]['topic'] = ldaTopicPredict(lda, validPreprocessed[x]['body'], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out_dataset = pd.DataFrame(validPreprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(out_dataset, open( \"pandas_processed_topics.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
